{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "icp10_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMZfs2BuafDE"
      },
      "source": [
        "Design a Deep Q Learning Network (DQN) using Keras and OpenAI Gym, for cartole game and visualize your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDt4Ng5XiyUc"
      },
      "source": [
        "Import the required libraries/packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6hDynmYjfa-"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJYV3S0ZiDTp"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyY6Fw1OlxCP"
      },
      "source": [
        "from collections import deque"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2TQFDAo4k4S"
      },
      "source": [
        "import math\n",
        "import random"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tKvbp0QjrYL",
        "outputId": "9d20722a-dc2c-45d6-e3eb-5dfdcebeecc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fc037caa278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3vWkPrai5QF"
      },
      "source": [
        "Setting up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn1duNnS0Im3"
      },
      "source": [
        "For the starting state all the observations are assigned a uniform random value between +- 0.05"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_bQs--xj2oy"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6zBb4T9z6MI"
      },
      "source": [
        "The action contains to discrete values. 0 and 1. 0 is to push the cart to the left and 1 is to push the cart to the right "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaORrxbHndvz",
        "outputId": "d832212e-4ac1-4eb1-ab04-f01d9bce8c91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "action_space = env.action_space.n\n",
        "print(action_space)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YG2ZNLRzsef"
      },
      "source": [
        "The observation consists of 4 values. Cart postion, Cart velocity, Pole Angle, and Pole Velocity At Tip. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNTNI-VAnRUU",
        "outputId": "5e5e703e-289b-4ec3-f3ba-bfe19c6f75ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "observation_space= env.observation_space.shape[0]\n",
        "print(observation_space)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNNtl2xKoAUs",
        "outputId": "427618f3-0aad-41a4-8807-86366ecb6443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "'''env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50):\n",
        "  action_space = env.action_space.sample()\n",
        "  observation_space, reward, done, info = env.step(action_space)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"env.reset()\\nprev_screen = env.render(mode='rgb_array')\\nplt.imshow(prev_screen)\\n\\nfor i in range(50):\\n  action_space = env.action_space.sample()\\n  observation_space, reward, done, info = env.step(action_space)\\n  screen = env.render(mode='rgb_array')\\n\\n  plt.imshow(screen)\\n  ipythondisplay.clear_output(wait=True)\\n  ipythondisplay.display(plt.gcf())\\n\\n  if done:\\n    break\\n\\nipythondisplay.clear_output(wait=True)\\nenv.close()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1HB31qm0hob",
        "outputId": "db2d09dd-3a5d-44f5-9e53-ae6f3d62f338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(5):\n",
        "  observation = env.reset()\n",
        "  for j in range(10):\n",
        "    env.render()\n",
        "    action_space = env.action_space.sample()\n",
        "    observation_space, reward, done, info = env.step(action_space)\n",
        "    if done:\n",
        "      print(\"Episode finished after {} timesteps\".format(j+1))\n",
        "      break\n",
        "\n",
        "  \n",
        "    screen = env.render(mode='rgb_array')\n",
        "\n",
        "    plt.imshow(screen)\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    plt.imshow(prev_screen)\n",
        "plt.imshow(prev_screen)\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH3YW3LD51QI"
      },
      "source": [
        "Setting up the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnvRuGjl6JZa"
      },
      "source": [
        "episodes = 50\n",
        "batch_size = 10\n",
        "max_env_steps = None\n",
        "\n",
        "gamma = 0.95 #discount factor. Consideration of future rewards\n",
        "learning_rate = 0.01 #most recent result \n",
        "memory_size = deque(maxlen=100000)\n",
        "\n",
        "explorationMax = 1.0 #choses uniformaly random choice (which way to go)\n",
        "explorationMin = 0.01\n",
        "explorationDecay = 0.995 #how fast the expoloration will stop\n",
        "\n",
        "if max_env_steps is not None:\n",
        "  env.max_episode_steps = max_env_steps"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "437zTcvM8zs4"
      },
      "source": [
        "Building the sequential model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yTj1Ojy82Ky",
        "outputId": "358dda26-021d-4a9f-8998-cb49683e0060",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#tree dense layers\n",
        "model = Sequential()\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(env.observation_space.shape[0],), activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(env.action_space.n, activation='linear'))\n",
        "model.compile(loss='mse', optimizer=Adam(lr=learning_rate,decay=explorationDecay), metrics=['mae'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 128)               640       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 9,026\n",
            "Trainable params: 9,026\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G_WSmYQ--RL"
      },
      "source": [
        "Defining functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iIOE4m-Aq-1"
      },
      "source": [
        "Remeber function will set up the momory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSS742zI_Ez6"
      },
      "source": [
        "def remember(state, action_space, reward, next_state, done):\n",
        "  memory.append((state,action_space,reward, next_state, done))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83YtgHWzAeYP"
      },
      "source": [
        "This functon is used to determine what will happen based on the exploration rate (max). It will change depending on the value of exploration max.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dK5Xb8z_kyL"
      },
      "source": [
        "def pick_action(state, explorationMax):\n",
        "  if (np.random.random) <= explorationMax:\n",
        "    return env.action_space.sample()\n",
        "\n",
        "  #else predict based on the current staage of the model if it is in the current stage\n",
        "  else:\n",
        "    np.argmax(model.predict(state))\n",
        "                                       "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TldLylFwHPo4"
      },
      "source": [
        "def get_exploration(t):\n",
        "  return max(explorationMin, min(explorationMax, 1.0 - math.log10((t+1)*explorationDecay)))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs6VVGYXBgsI"
      },
      "source": [
        "preprocess to make sure it is in the right format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgkd4gFKBpXI"
      },
      "source": [
        "def pre_State(state):#prepocess state\n",
        "  return np.reshape(state,[1,4])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_hnoqzdB5i5"
      },
      "source": [
        "replay function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_72QXJYB-J5"
      },
      "source": [
        "def replay_fun(batch_size, explorationMax):\n",
        "  batch_x = []\n",
        "  batch_y = []\n",
        "  smallBatch = random.sample(memory_size, min(len(memory_size),batch_size))\n",
        "\n",
        "  for state, action_space, reward, next_state, done in smallBatch:\n",
        "    target_y = model.predict(state) #what the model will predict\n",
        "    if done:\n",
        "      reward\n",
        "    else:\n",
        "      reward + gamma + np.max(model.predict(next_state)[0])\n",
        "\n",
        "    batch_x.append(state[0])\n",
        "    batch_y.append(state[0])\n",
        "    model.fit(np.array(batch_x),batch_size=len(batch_x),verbose=0)\n",
        "\n",
        "    #update the exploration each time\n",
        "    if explorationMax > explorationMin:\n",
        "      explorationMax *= explorationDecay #decreases each time the replay fun is called"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFpXwceMIh_z"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpadpgbgGCkA"
      },
      "source": [
        "#this will record the environemnt state and will pick the best action\n",
        "def run():\n",
        "  scores = deque(maxlen=100)\n",
        "\n",
        "  for num_episodes in range(episodes):\n",
        "    state = pre_State(env.reset()) #start from the begining each time\n",
        "    done = False\n",
        "    time_step = 0\n",
        "    while not done:\n",
        "      action_space = pick_action(state, get_exploration(num_episodes))\n",
        "      next_stat, reward, done, _= env.step(action_space)\n",
        "      env.render()\n",
        "      next_state = pre_State(next_state)\n",
        "      remember(state, action_space, reward, next_state, done)\n",
        "      state = next_state\n",
        "\n",
        "    scores.append(time_step)\n",
        "    mean_score = np.mean(scores)\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KkBp01SLiUm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgBfyuxrLmnw",
        "outputId": "aec5ef1a-58bb-4b72-b878-6e4b2af75183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from collections import deque\n",
        "import math\n",
        "import random\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "action_space = env.action_space.n\n",
        "print(action_space)\n",
        "\n",
        "observation_space= env.observation_space.shape[0]\n",
        "print(observation_space)\n",
        "\n",
        "env.reset()\n",
        "#prev_screen = env.render(mode='rgb_array')\n",
        "#plt.imshow(prev_screen)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "episodes = 50\n",
        "batch_size = 10\n",
        "max_env_steps = None\n",
        "\n",
        "gamma = 0.95 #discount factor. Consideration of future rewards\n",
        "learning_rate = 0.01 #most recent result \n",
        "memory_size = deque(maxlen=100000)\n",
        "\n",
        "explorationMax = 1.0 #choses uniformaly random choice (which way to go)\n",
        "explorationMin = 0.01\n",
        "explorationDecay = 0.995 #how fast the expoloration will stop\n",
        "win_ticks = 200\n",
        "\n",
        "if max_env_steps is not None:\n",
        "  env.max_episode_steps = max_env_steps\n",
        "\n",
        "#three dense layers\n",
        "model = Sequential()\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(env.observation_space.shape[0],), activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(env.action_space.n, activation='linear'))\n",
        "model.compile(loss='mse', optimizer=Adam(lr=learning_rate,decay=explorationDecay), metrics=['mae'])\n",
        "print(model.summary())\n",
        "\n",
        "def remember(state, action_space, reward, next_state, done):\n",
        "  memory.append((state,action_space,reward, next_state, done))\n",
        "\n",
        "def pick_action(state, explorationMax):\n",
        "  if (np.random.random) <= (explorationMax):\n",
        "    return env.action_space.sample()\n",
        "\n",
        "  #else predict based on the current staage of the model if it is in the current stage\n",
        "  else:\n",
        "    np.argmax(model.predict(state))\n",
        "\n",
        "def get_exploration(t):\n",
        "  return max(explorationMin, min(explorationMax, 1.0 - math.log10((t+1)*explorationDecay)))\n",
        "\n",
        "def pre_State(state):#prepocess state\n",
        "  return np.reshape(state,[1,4])\n",
        "\n",
        "def replay_fun(batch_size, explorationMax):\n",
        "  batch_x = []\n",
        "  batch_y = []\n",
        "  smallBatch = random.sample(memory_size, min(len(memory_size),batch_size))\n",
        "\n",
        "  for state, action_space, reward, next_state, done in smallBatch:\n",
        "    target_y = model.predict(state) #what the model will predict\n",
        "    if done:\n",
        "      reward\n",
        "    else:\n",
        "      reward + gamma + np.max(model.predict(next_state)[0])\n",
        "\n",
        "    batch_x.append(state[0])\n",
        "    batch_y.append(state[0])\n",
        "    model.fit(np.array(batch_x),batch_size=len(batch_x),verbose=0)\n",
        "\n",
        "    #update the exploration each time\n",
        "    if explorationMax > explorationMin:\n",
        "      explorationMax *= explorationDecay #decreases each time the replay fun is called\n",
        "\n",
        "#this will record the environemnt state and will pick the best action\n",
        "#this will record the environemnt state and will pick the best action\n",
        "def run():\n",
        "  scores = deque(maxlen=100)\n",
        "\n",
        "  for num_episodes in range(episodes):\n",
        "    state = pre_State(env.reset()) #start from the begining each time\n",
        "    done = False\n",
        "    time_step = 0\n",
        "    while not done:\n",
        "      action_space = pick_action(state, get_exploration(num_episodes))\n",
        "      next_stat, reward, done, _= env.step(action_space)\n",
        "      env.render()\n",
        "      next_state = pre_State(next_state)\n",
        "      remember(state, action_space, reward, next_state, done)\n",
        "      state = next_state\n",
        "\n",
        "    scores.append(time_step)\n",
        "    mean_score = np.mean(scores)\n",
        "\n",
        "\n",
        "\n",
        "run()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "4\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 128)               640       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 9,026\n",
            "Trainable params: 9,026\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-96d77bafeb49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-96d77bafeb49>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpick_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_exploration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0mnext_stat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-96d77bafeb49>\u001b[0m in \u001b[0;36mpick_action\u001b[0;34m(state, explorationMax)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpick_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplorationMax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexplorationMax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'builtin_function_or_method' and 'float'"
          ]
        }
      ]
    }
  ]
}